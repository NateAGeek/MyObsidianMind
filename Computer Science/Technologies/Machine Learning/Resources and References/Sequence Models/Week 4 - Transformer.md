Transformer: Mix of attention and CNN
Self-Attention
Multi-Head Attention

## Self-Attention
Compute an attention base repetition of each word. It basically looks around the words and determine its contexts of the word. For example, base on where Charles is being used, you could say it's a name but if street was around it then it might be "Charles St". This gives different representation of the words, and thus the attention patter might have a different context of it.

Query, Key, and Value. These variables are used to calculate the attention value for each word. 

