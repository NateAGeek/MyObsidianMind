## Activation Functions
Sigmoid has been used to try and fit features for binary values. Like someone likes something or not. However, there is the strong possibility that there may not be as straight a forward probability. That can lead to maybe different influences. There is the ReLU function that takes approach from 0 to 1.

## Choosing Activation Functions

We use for our output nodes we can choose the basic

#### Binary Classifications
Sigmoid are used for binary classification since it sets a value as a probability of between
#### Regression Classification
Linear Activation are used for values that can be both positive or negative when 
ReLU for positive numbers

_ReLU has become the common activation function for training hidden layers as it can be faster at resolving_

The reason to default use ReLu over Linear is due to linear would just lead to linear recession rather than a fitting of the data. ReLUs have the ability to turn on and off function when needed. Since their activation 