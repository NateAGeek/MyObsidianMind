## Overview
The paper discusses the way some neurons in models pack unrelated concepts into a single neuron, kinda like a compression of multiple data sets into one node. They call this "polysemanticity" of neurons.

### Introduction
The paper discusses how models, toy models in their case, encode relatively overlapping data as more features are introduced. They use the concepts of superposition and the interference principles to encode more sparsely data into one neuron. Allowing for the neuron to contribute to compressing more relations than are linearly learned. 
![[../../../../NotebookAssets/Pasted image 20240527223408.png]]

They then also lead to how this interference can lead to an almost "eureka" moment and explain how models that jump in learning and grokking, where the model generalized after overfitting,

**Key Results**
* They do demonstrate superposition is real
* Both monosemantic and polysemantic neurons can form
* computation on performance of superposition is possible
* Phase changes govern superposition of features
* Superposition can organize features into geometric structures

## Definitions and Motivations: Features, Directions, and Superposition

The sections defines some areas of introspection of how data is composed in these neurons. They define goals of defining what makes the neurons encode data. 

* **Decomposability**: The way features are independently understood within the neurons structures.
* **Linearity**: how the features are representing directionally in dimensional spaces. 
* **Privileged Basis**: The dimensionality of the features take higher priority in the encoded neuron, aka the neuron has more priority of the feature compared to other neurons. Like how some layers in covnets detect edges to identify faces (feature).
* **Superposition**: Are neurons that have multiple encoding of the features that compress relational encodings.

## Empirical Phenomena
They go over the current way "features" have been empirically discussed and discovered to encode information. Such as the following:
* **Word Embeddings**: How word embeddings become vector relations over large trainings. The vector relations contain a degree of relation, such as V("king") - V("man") + V("woman") = V("queen")
* **Latent Spaces**: It is the dimensional vector encoding of data within the neurological structure. 
* **Interpretable Neurons**: Neurons that give specific activations that are Interpretable to the basis of humans. Such as RNN and CNNs giving clear activations for edges or contexts.
* **Universality**: That multiple neurons can contain the same properties later in the network, as in stagnation of multiple points of identification.
* **Polysemantic Neurons**: Are neurons that can respond and activate to different mixtures of inputs to affect and conclude different outputs. 
## What are features?
The papers defines some general definitions for what a feature could be in many models. However, it does not attach an ideology on an absolute of what a feature is. 

* **Features as Functions**: Are defining features as encodings or abstractions of data as vector.
* **Interpretable Properties**: Clearly identifiable features that have an humanistic interpretability. Although note, some of these features might require some discovery especially if generated by models.
* **Neurons in Sufficiently Large Models**: Basically a neurons ability to compress features to their designated identifiable output. Like how CNNs can start making filters for edges.

## Features as Directions

The section discusses how neurons use direction in multi-dimensional space to store features and pass them to following neurons. They use linear representations to fit and find the data. They give reasoning to these linear abilities as follows:
* **Natural Outputs**: How most data tend to follows linear representations and feature extraction layer by layer. 
* **Linearly Accessible**: How features are subsequently influence in a positive or negative feedback to other layer neurons.
* **Statistical Efficiency**: Giving neurons vector like directions allows for more non-local generalization, and can provide efficiency in generalization. 
They then lead to how people believe there might be a tight relation between dimension of features and the neuron encoding. However, that is not the case as they will demonstrate "superposition" of neurons encoding more features. 
## Privileged vs Non-privileged Bases
Privileged and non-privileged both exist within models in this context. However, they do have different abilities.

**Privileged**: Means that the features are aligned with in activation with specific features. So, neurons are able to clearly show activation with feature inputs. This allows us to do Symmetry Breaking to making certain directions more meaningful. 

**Non-privileged**: Means the directional relations in the activations are arbitrary, and any relational direction could be significant. Word Embeddings are an example of features that are defined manually.

## The Superposition Hypothesis
The hypothesis suggests that there are almost-orthogonal encodings in the neuron to allow the neuron to store more feature relations. 

They way that this happens is when the representation of the vectors are almost orthogonal. This leads to the network to encode more features.

Also, neurons tends to want to compress the data and relations, due to the loss functionality. Allowing the neuron to almost sense compression vectors. 

Although there might be an assumed cost to compressing features into neurons, through "noise" or "interference" of directional relations. The benefit might out weight the cost, as highly sparse features, allowing them to encode more features in one form factor in a single neuron with lower dimensionality. 

## Summary: Of Hierarchy of Features Properties

* **Decomposability**: Activations can be decomposed into features, meaning that activations of specific neurons can be observed to activate their encoded features.
* **Linearity**: Each feature has a relational direction
* Superposition vs Non-Superposition: Superposition exists if $W^TW$ is not invertible, if it is then it is in non-superposition.
* **Basis-Aligned**: TODO: REVIEW WITH CHATGPT...
## Demonstrating Superposition

On a basic linear model we are storing principal components. However, when we add nonlinearity in our models, we then get different super positions.

## Experiment Setup

The paper makes the assertion that a larger model should be able to encode the relation of features to the output with large accuracy via a dense network. However, the experiment objectively should be able to down project, and ultimately encode the features in a reduced state, and thus create superpositions.
![[../../../../NotebookAssets/Pasted image 20240701174259.png]]

### The feature Vector ($X$)
They create synthetic data for the features to learn. This is due to the idea that there is not a foundational most baseline of features that does not have some subjective reasoning. Ie, Visional model, curve fitting, Gabor filter, floppy ear detection... etc. 

##### Feature Design
* **Feature Sparsity**: in the "natural world" data seems to be sparsity related. Meaning the relation of data across multiple domains lack clear relation. Position of dogs may not be aligned perfectly in vision models. Language models may not always talk about cats, so the relation of the data is quite far. 
* **More Features than Neurons**: The number of neurons and features are not the same in most models, and more features are usually available than a neuron can really utilized (aka data is everywhere).
* **Features vary in importances**: Not all features are relatively important as others. Some can reduce losses more than others and subsequently lead to an efficient the why that neurons learn. For example in the imageNet model, there is a stronger relation of dog species detection with floppy ears than others. 
###### Fine Details of Synthetic Features
The models defines a series of vectors $x_i$ each one has an associated degree of sparsity, $S_i$, and importances,$I_i$. The data Sparsity is uniformity distributed between $[0, 1]$.

### The Model ($X -> X^{'}$)
They utilize two models in their experiments. One is a basic linear model and the other is a ReLU model. The objective of the models is to have a baseline of a model that does not exhibit superposition. the linear models are know to not have superpositions, this is due to the structure of the model lining up with the data directly. While a ReLu function has the potential to represent complex or operations and fit data none linearly. 

We are given an input as $x$ in a higher dimensional space and we want to compress its features into lower dimensional space using a weighted matrix $W$ to a vector $h$

However, to recover the original input vector $x$ we need to need to use the $W^T$ to inverse the $h$ relation. We add a bias to account for possible missing values as $b$.

#### Differences in Recovery of Information Between Models

**Linear Models**: The linear models is basically focusing on the Orthogonal components of the data. This is due to the nature of a linear model not able to learn non-linear relation, and thus will only have representation of prominent directions and features along the main axis. It is basically a PCA projection, capturing only the most prominent directions and features of the input data.

**ReLU Model**: The ReLU activation function adds complexity in recovering the original $x$ vector due to the complexity in transformations due to non-linearity. This allows the $W$ to compress and capture features. This allows the neurons to better store complex relations and features, this results in more sparser and complex feature interactions that are harder to disentangle.

## Basic Results
They analyzed the results via the comparing a linear and relu model. The properties used 20 neurons and 5 features. The linear model learned direct relations of features, and there was no encoding of superpositions of the data. The 