## Overview
The paper discusses the way some neurons in models pack unrelated concepts into a single neuron, kinda like a compression of multiple data sets into one node. They call this "polysemanticity" of neurons.

### Introduction
The paper discusses how models, toy models in their case, encode relatively overlapping data as more features are introduced. They use the concepts of superposition and the interference principles to encode more sparsely data into one neuron. Allowing for the neuron to contribute to compressing more relations than are linearly learned. 
![[../../../../NotebookAssets/Pasted image 20240527223408.png]]

They then also lead to how this interference can lead to an almost "eureka" moment and explain how models that jump in learning and grokking, where the model generalized after overfitting,

**Key Results**
* They do demonstrate superposition is real
* Both monosemantic and polysemantic neurons can form
* computation on performance of superposition is possible
* Phase changes govern superposition of features
* Superposition can organize features into geometric structures

## Definitions and Motivations: Features, Directions, and Superposition

The sections defines some areas of introspection of how data is composed in these neurons. They define goals of defining what makes the neurons encode data. 

* **Decomposability**: The way features are independently understood within the neurons structures.
* **Linearity**: how the features are representing directionally in dimensional spaces. 
* **Privileged Basis**: The dimensionality of the features take higher priority in the encoded neuron, aka the neuron has more priority of the feature compared to other neurons. Like how some layers in covnets detect edges to identify faces (feature).
* **Superposition**: Are neurons that have multiple encoding of the features that compress relational encodings.

## Empirical Phenomena
They go over the current way "features" have been empirically discussed and discovered to encode information. Such as the following:
* **Word Embeddings**: How word embeddings become vector relations over large trainings. The vector relations contain a degree of relation, such as V("king") - V("man") + V("woman") = V("queen")
* **Latent Spaces**: It is the dimensional vector encoding of data within the neurological structure. 
* **Interpretable Neurons**: Neurons that give specific activations that are Interpretable to the basis of humans. Such as RNN and CNNs giving clear activations for edges or contexts.
* **Universality**: That multiple neurons can contain the same properties later in the network, as in stagnation of multiple points of identification.
* **Polysemantic Neurons**: Are neurons that can respond and activate to different mixtures of inputs to affect and conclude different outputs. 
## What are features?
The papers defines some general definitions for what a feature could be in many models. However, it does not attach an ideology on an absolute of what a feature is. 

* **Features as Functions**: Are defining features as encodings or abstractions of data as vector.
* **Interpretable Properties**: Clearly identifiable features that have an humanistic interpretability. Although note, some of these features might require some discovery especially if generated by models.
* **Neurons in Sufficiently Large Models**: Basically a neurons ability to compress features to their designated identifiable output. Like how CNNs can start making filters for edges.

## Features as Directions

The section discusses how neurons use direction in multi-dimensional space to store features and pass them to following neurons. They use linear representations to fit and find the data. They give reasoning to these linear abilities as follows:
* **Natural Outputs**: How most data tend to follows linear representations and feature extraction layer by layer. 
* **Linearly Accessible**: How features are subsequently influence in a positive or negative feedback to other layer neurons.
* **Statistical Efficiency**: Giving neurons vector like directions allows for more non-local generalization, and can provide efficiency in generalization. 
They then lead to how people believe there might be a tight relation between dimension of features and the neuron encoding. However, that is not the case as they will demonstrate "superposition" of neurons encoding more features. 
## Privileged vs Non-privileged Bases
Privileged and non-privileged both exist within models in this context. However, they do have different abilities.

**Privileged**: Means that the features are aligned with in activation with specific features. So, neurons are able to clearly show activation with feature inputs.

**Non-privileged**: Means the directional relations in the activations are arbitrary, and any relational direction could be significant. 

