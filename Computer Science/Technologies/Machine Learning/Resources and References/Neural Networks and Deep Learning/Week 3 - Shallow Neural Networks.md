## Activation Functions
Basically goes over how the sigmoid function is slower due to how it might average to .5 rather allowing the averages to mean to 0 - 1. Using Relu is a good activation function...

## Why do we use Activation Functions
If we just did linear functions, we could simplify it into one big one and it would not really provide any value in fitting the data.
